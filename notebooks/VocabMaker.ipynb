{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import punkt # для сегментации предложений\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re #для замены ненужных символов\n",
    "from collections import Counter\n",
    "import re\n",
    "from string import punctuation\n",
    "punct = punctuation+'«»—…“”*№–'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabMaker(object):\n",
    "    def __init__(self, corpus, min_freq=2):\n",
    "        self.corpus = self.read_corpus(corpus)\n",
    "        self.corpus_words, self.num_corpus_words, self.counts = self.distinct_words(min_freq)\n",
    "        self.index2word, self.word2index = self.build_index()\n",
    "               \n",
    "    def read_corpus(self, filename, min_freq=20):\n",
    "        print('Reading a corpus')\n",
    "        if str(filename).endswith('.txt'):\n",
    "            file = open(filename, 'r', encoding = 'utf-8')\n",
    "            return([[ re.sub('{[^}]}', '', w.lower()) for w in nltk.tokenize.WordPunctTokenizer().tokenize(sentence)] \n",
    "                    for sentence in sent_tokenize(file.read(), language='slovene')])\n",
    "            file.close()\n",
    "            print('Done!')\n",
    "        else:\n",
    "            return(filename)\n",
    "        \n",
    "    def not_russian(self, string):\n",
    "        s = re.sub(\"[.,:\\'-]\", '', string)\n",
    "        charRe = re.compile(r'[a-zA-Z0-9.]')\n",
    "        st = charRe.search(s)\n",
    "        return bool(st) or bool(re.search(r'\\d', s) or bool(re.search(r'[^a-zа-яёіѳѣ ]+', string)))\n",
    "    \n",
    "    def distinct_words(self, min_freq):\n",
    "        print('Estimating cospus size')\n",
    "        flat_corpus = [word for sentence in self.corpus for word in sentence]\n",
    "        counts = Counter(flat_corpus)\n",
    "        corpus_words = sorted(list(set([word for word in flat_corpus if not self.not_russian(word) and counts[word]> min_freq])))\n",
    "        num_corpus_words = len(corpus_words)\n",
    "        \n",
    "        return(corpus_words, num_corpus_words, counts)\n",
    "        print('Done!')\n",
    "        print('You can checkout the number of corpus words and the words themselves with commands .num_corpus_words, .corpus_words')\n",
    "    \n",
    "    def build_index(self):\n",
    "        word2index = dict()\n",
    "        for word in self.corpus_words:\n",
    "            word2index[word] = len(word2index)\n",
    "            word2index['UNK'] = 0\n",
    "        return ({index:word for word, index in word2index.items()}, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading a corpus\n",
      "Estimating cospus size\n"
     ]
    }
   ],
   "source": [
    "VM = VocabMaker(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1116367"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VM.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346548"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VM.corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading a corpus\n",
      "Estimating cospus size\n"
     ]
    }
   ],
   "source": [
    "VM2 = VocabMaker(\"corpus2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6852"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VM2.corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading a corpus\n",
      "Estimating cospus size\n"
     ]
    }
   ],
   "source": [
    "VM3 = VocabMaker(\"corpus3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55787"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VM3.corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading a corpus\n",
      "Estimating cospus size\n"
     ]
    }
   ],
   "source": [
    "VM_final = VocabMaker([word for sentence in VM.corpus+VM2.corpus+VM3.corpus for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = [word for sentence in VM.corpus+VM2.corpus+VM3.corpus for word in sentence]\n",
    "counts = Counter(flat_corpus)\n",
    "corpus_words = sorted(list(set([word for word in flat_corpus if not VM.not_russian(word) and counts[word]>2])))\n",
    "num_corpus_words = len(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Итоговое количество токенов, уникальных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42793167"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VM_final.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212605"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VM_final.num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['а',\n",
       " 'аа',\n",
       " 'аабаан',\n",
       " 'аава',\n",
       " 'аадур',\n",
       " 'аалѣться',\n",
       " 'аарау',\n",
       " 'аарона',\n",
       " 'аарону',\n",
       " 'ааронъ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VM_final.corpus_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Добавление в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('counts.txt', 'a') as f:\n",
    "    for word in list(VM_final.counts.keys()):\n",
    "        f.write(\"{} {}\\n\".format(word, counts[word]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt', 'a') as f:\n",
    "    for word in VM_final.corpus_words:\n",
    "        f.write(\"{}\\n\".format(word))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = open('vocabulary.txt', 'r').read().split(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
