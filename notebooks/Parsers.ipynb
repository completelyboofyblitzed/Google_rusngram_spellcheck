{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArhivarijParser(object):\n",
    "    def __init__(self):\n",
    "        self.link = \"http://arhivarij.narod.ru/\"\n",
    "        self.links = []\n",
    "        self.alphabeth = 'í́ІАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяіѢѣѲѳѴѵ'\n",
    "        self.restrict = 0\n",
    "        self.allarticles = \"\"\n",
    "\n",
    "    def _get_links(self, link):\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                resp = requests.get(link)\n",
    "                bss = BeautifulSoup(resp.text, \"html5lib\")\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                print('failed')\n",
    "                self.restrict+=1\n",
    "                if self.restrict>200:\n",
    "                    break\n",
    "                \n",
    "        try:\n",
    "            for a in bss.find_all(\"a\"):\n",
    "                try:\n",
    "                    start_link = str(a).find('http://')\n",
    "                    end_link = str(a).find('.html')+5\n",
    "                    link = str(a)[start_link:end_link]\n",
    "                    if link not in self.links:\n",
    "                        self.links.append(link)\n",
    "                except:\n",
    "                    print('Что-то не так с получением ссылок')\n",
    "\n",
    "                print('Внимание! Обнаружено ', len(self.links), ' статей!\\r', end='')\n",
    "        except:\n",
    "            print('Видимо не нашлась ссылка')\n",
    "        print()\n",
    "\n",
    "    def get_links(self):\n",
    "        \n",
    "        print('Обрабатываю ')\n",
    "        \n",
    "        self._get_links(self.link)\n",
    "        \n",
    "        print('Обрабатываю еще')\n",
    "        for link in self.links:\n",
    "            self._get_links(link)\n",
    "            if len(self.links) >3000:\n",
    "                break\n",
    "\n",
    "    def _get_article(self, link):\n",
    "        \"\"\"\n",
    "        Парсим тело статьи\n",
    "        \"\"\"\n",
    "        resp = requests.get(link)\n",
    "        bs = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        raw_article = ' '.join([re.sub('\\s+',\n",
    "                                   ' ',\n",
    "                                   re.sub(r\"[^0-9í́ІIАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяіiѢѣѲѳѴѵ.,]\",\n",
    "                                          ' ',\n",
    "                                          str(e))).strip() for e in bs()])\n",
    "#         print('got raw article')\n",
    "        counts=Counter(list(raw_article))\n",
    "#         print('got counts')\n",
    "        i_counts = counts['i']\n",
    "        I_counts = counts['I']\n",
    "#         print('counts of i = {}\\ncounts of I = {}'.format(i_counts, I_counts))\n",
    "        \n",
    "        for  e in range(i_counts):\n",
    "            if raw_article[raw_article.find('i')+1] in self.alphabeth or raw_article[raw_article.find('i')+1] in self.alphabeth:\n",
    "                raw_article=raw_article[:raw_article.find('i')]+'і'+raw_article[raw_article.find('i')+1:]\n",
    "            else: \n",
    "                raw_article=raw_article[:raw_article.find('i')]+raw_article[raw_article.find('i')+1:]\n",
    "                \n",
    "        raw_article = raw_article.replace(' .', '')\n",
    "        \n",
    "        for  e in range(I_counts):\n",
    "            if raw_article[raw_article.find('I')+1] in self.alphabeth or raw_article[raw_article.find('I')+1] in self.alphabeth:\n",
    "                raw_article=raw_article[:raw_article.find('I')]+'І'+raw_article[raw_article.find('I')+1:]\n",
    "            else: \n",
    "                raw_article=raw_article[:raw_article.find('I')]+raw_article[raw_article.find('I')+1:]\n",
    "        article = re.sub('\\s+',' ',raw_article)\n",
    "        return article.replace(' ,', ',').replace(',,', ',')\n",
    "\n",
    "    def get_all_articles(self):\n",
    "        \"\"\"\n",
    "        Проходим по всем статьям и парсим их\n",
    "        \"\"\"\n",
    "        links = []  # список статей\n",
    "        count = 0  # это показатель, сколько статей обработано (т.к. все достаточно медленно)\n",
    "        success = 0\n",
    "        for link in self.links:\n",
    "            lk = link\n",
    "            try:\n",
    "                self.allarticles += self._get_article(lk)\n",
    "                success += 1\n",
    "            except:\n",
    "                print(\"didn't get article\")\n",
    "                pass\n",
    "            count += 1\n",
    "            if success>360:\n",
    "                break\n",
    "            print('Обработано статей: {} Успешно: {}'.format(count, success))\n",
    "\n",
    "    \n",
    "    def to_txt(self):\n",
    "        \"\"\"\n",
    "        Записываем в файл\n",
    "        \"\"\"\n",
    "        with open('corpus3.txt', 'a', encoding='utf-8-sig') as f:\n",
    "            f.write(self.allarticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'I'=='І'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RusPortalParser(object):\n",
    "    def __init__(self):\n",
    "        self.link = \"http://www.russportal.ru/\"\n",
    "        self.links = []\n",
    "        self.restrict = 0\n",
    "\n",
    "    def _get_links(self, link):\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                resp = requests.get(link)\n",
    "                bss = BeautifulSoup(resp.text, \"html5lib\")\n",
    "                break\n",
    "            except:\n",
    "                print('failed')\n",
    "                self.restrict+=1\n",
    "                if self.restrict>50:\n",
    "                    break\n",
    "        try:\n",
    "            for element in bss.find_all(\"div\"):\n",
    "                try:\n",
    "                    link = element.find(\"a\")[\"href\"]\n",
    "                    if 'http' in link:\n",
    "                        if link not in self.links:\n",
    "                            self.links.append(link)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                self.links.append(link)\n",
    "                print('Внимание! Обнаружено ', len(self.links), ' статей!\\r', end='')\n",
    "            print()\n",
    "        except:\n",
    "            print('Видимо не нашлась ссылка')\n",
    "\n",
    "    def get_links(self):\n",
    "\n",
    "        print('Обрабатываю ')\n",
    "        self._get_links(self.link)\n",
    "        for lk in self.links:\n",
    "            self._get_links(lk)\n",
    "            if len(self.links) >20000:\n",
    "                break\n",
    "\n",
    "    def _get_article(self, link):\n",
    "        \"\"\"\n",
    "        Парсим тело статьи\n",
    "        \"\"\"\n",
    "        resp = requests.get(link)\n",
    "        bs = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        article = ' '.join([re.sub('\\s+',\n",
    "                                   ' ', \n",
    "                                   re.sub(r\"[^0-9í́ІАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяіѢѣѲѳѴѵ.,-]\", \n",
    "                                          ' ',\n",
    "                                          str(e))).strip() for e in bs('p')])\n",
    "        return article\n",
    "\n",
    "    def get_all_articles(self):\n",
    "        \"\"\"\n",
    "        Проходим по всем статьям и парсим их\n",
    "        \"\"\"\n",
    "        links = []  # список статей\n",
    "        self.allarticles = \"\"\n",
    "        count = 0  # это показатель, сколько статей обработано (т.к. все достаточно медленно)\n",
    "        success = 0\n",
    "        for link in self.links:\n",
    "            lk = link\n",
    "            try:\n",
    "                self.allarticles += re.sub('\\s+',' ', self._get_article(lk))\n",
    "                success += 1\n",
    "            except:\n",
    "                pass\n",
    "            count += 1\n",
    "            if success>10000:\n",
    "                break\n",
    "            print('Обработано статей:', count, ' Успешно:', success, '\\r', end='')\n",
    "\n",
    "    \n",
    "    def to_txt(self):\n",
    "        \"\"\"\n",
    "        Записываем в файл\n",
    "        \"\"\"\n",
    "        with open('corpus2.txt', 'a', encoding='utf-8-sig') as f:\n",
    "            f.write(self.allarticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = RusPortalParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ArhivarijParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my.get_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A.get_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.53 s, sys: 1.7 s, total: 5.23 s\n",
      "Wall time: 1h 18min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my.get_all_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 183\n",
    "# 4871\n",
    "# 2990000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A.get_all_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16282486"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.sub('\\s+',' ',my.allarticles).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 766 ms, sys: 1.22 s, total: 1.98 s\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my.to_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 125 ms, sys: 125 ms, total: 250 ms\n",
      "Wall time: 531 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "A.to_txt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
